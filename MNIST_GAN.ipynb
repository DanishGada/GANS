{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1X_qhWwavsrCChQJ27ZvyN7E2fEwQOdzi",
      "authorship_tag": "ABX9TyPCpFR6JHbXS1I0ZxZZifVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanishGada/GANS/blob/main/MNIST_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlN5R5_l0k3t"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "matplotlib.style.use('ggplot')\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpLFOhkE1MJ6"
      },
      "source": [
        "batch_size = 1024\n",
        "epochs = 200\n",
        "sample_size = 64 # fixed sample size\n",
        "nz = 128 # latent vector size\n",
        "k = 1 # number of steps to apply to the discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMoSmEeX26Z3"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,)),\n",
        "])\n",
        "to_pil_image = transforms.ToPILImage()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0OmwS3d2-XV"
      },
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root='../input/data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpvmXxr83An8"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.nz, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc4OJqST3I5G"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_input = 784\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.n_input, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        return self.main(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws1tmdv03MDZ"
      },
      "source": [
        "generator = Generator(nz)\n",
        "discriminator = Discriminator()\n",
        "print('##### GENERATOR #####')\n",
        "print(generator)\n",
        "print('######################')\n",
        "print('\\n##### DISCRIMINATOR #####')\n",
        "print(discriminator)\n",
        "print('######################')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl6QqAV93PZ_"
      },
      "source": [
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwXnbAdg3m2Z"
      },
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovr0t8um3oUC"
      },
      "source": [
        "losses_g = [] \n",
        "losses_d = [] \n",
        "images = [] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3qSw0x73qk3"
      },
      "source": [
        "def label_real(size):\n",
        "    data = torch.ones(size, 1)\n",
        "    return data\n",
        "    \n",
        "def label_fake(size):\n",
        "    data = torch.zeros(size, 1)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIROZlkC3skS"
      },
      "source": [
        "def create_noise(sample_size, nz):\n",
        "    return torch.randn(sample_size, nz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkaU0zxQ32OG"
      },
      "source": [
        "def save_generator_image(image, path):\n",
        "    save_image(image, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhJugrT4El4"
      },
      "source": [
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "    fake_label = label_fake(b_size)\n",
        "    optimizer.zero_grad()\n",
        "    # sEND REAL data\n",
        "    output_real = discriminator(data_realx`)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "    #Send fake data\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "    #backprop\n",
        "    loss_real.backward()\n",
        "    loss_fake.backward()\n",
        "    optimizer.step()\n",
        "    return loss_real + loss_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pegCMhDP4G2T"
      },
      "source": [
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "    optimizer.zero_grad()\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51jcS0BT4InX"
      },
      "source": [
        "noise = create_noise(sample_size, nz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efIa8_cT4Kxq"
      },
      "source": [
        "generator.train()\n",
        "discriminator.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShUpiaAK4NM9"
      },
      "source": [
        "epoch_loss_g=0\n",
        "epoch_loss_d=0\n",
        "\n",
        "'''\n",
        "        In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch \n",
        "        accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, \n",
        "        the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
        "        \n",
        "        Because of this, when you start your training loop, ideally you should zero out the gradients so that \n",
        "        you do the parameter update correctly. Else the gradient would point in some other direction than the \n",
        "        intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
        "        \n",
        "        '''\n",
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for bi, data in enumerate(train_loader):\n",
        "        image, _ = data\n",
        "        b_size = len(image)\n",
        "        if epoch_loss_g > 3 and abs(epoch_loss_d - 0.5) < 0.05:\n",
        "          k=1\n",
        "        else:\n",
        "          k=2\n",
        "        for step in range(k):\n",
        "            data_fake = generator(create_noise(b_size, nz)).detach()\n",
        "            data_real = image\n",
        "        \n",
        "        loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(create_noise(b_size, nz))\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "        print('.',end=\"\")\n",
        "    generated_img = generator(noise)\n",
        "    generated_img = make_grid(generated_img)\n",
        "    save_generator_image(generated_img, f\"/content/drive/MyDrive/Colab Notebooks/outputs2/gen_img{epoch}.png\")\n",
        "    images.append(generated_img)\n",
        "    epoch_loss_g = loss_g / bi \n",
        "    epoch_loss_d = loss_d / bi\n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "    \n",
        "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:}, Discriminator loss: {epoch_loss_d:}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}